\subsection{Teoria della stima}
Per stima si intende il processo di inferire il valore di una variabile casuale $X$ di interesse dall'osservazione di un'altra variabile casuale $Y$ che dipenda in qualche modo da $X$. Le variabili coinvolte sono pertanto:
\begin{itemize}
	\item la variabile da stimare $X \in \mathbb{R}^n$
	\item la variabile osservata $Y \in \mathbb{R}^p$
\end{itemize}

Si definisce \textit{stimatore di $X \in \mathbb{R}^n$ basato sull’osservazione $Y \in \mathbb{R}^p$} una funzione $g:\mathbb{R}^p \rightarrow \mathbb{R}^n$ operante sull'osservazione $Y$ per produrre una stima $\widehat{X}=g(Y)$.
Dato che le variabili in gioco sono casuali, anche la stima sarà una variabile casuale. Distinguiamo la \textit{variabile casuale stima} $\widehat{X}$ da $\hat{x}$ che è il particolare valore della stima ottenuto dall'applicazione dello stimatore all'osservazione $y$ effettuata.

Si definisce l'\textit{errore di stima} $\mathrm{E}=X-\widehat{X}=X-g(Y)$ come la differenza tra la variabile da stimare e la sua stima. Un obiettivo naturale è quello di rendere tale errore di stima "piccolo" in accordo
a qualche criterio, deterministico o probabilistico, da precisare opportunamente. A tale
proposito, un criterio valido potrebbe essere quello di avere un errore di stima a media
nulla (\textit{stimatore non polarizzato}):
\[E[\mathrm{E}]=E[X-\widehat{X}]=0 \implies E[\widehat{X}]=E[X]\]
e di covarianza $E[\mathrm{E}\mathrm{E}^T]$ in qualche modo minima.

Come indice di prestazione per valutare la qualità di uno stimatore si introduce l’errore quadratico medio (MSE = Mean Square Error):
\begin{equation}
MSE_g(y)= E[(X-g(Y))^T(X-g(Y))|Y=y]
\end{equation}

Poiché uno stimatore con MSE inferiore è certamente da preferirsi, viene naturale porsi il problema della determinazione dello stimatore a minimo errore quadratico medio(stimatore MMSE = \textit{Minimum Mean Squared Error}) ovvero di uno stimatore $g^*(Y)$ il cui MSE sia inferiore a quello di ogni altro stimatore $g(Y)$.

\subsubsection{Stimatore MMSE}
Lo stimatore MMSE $g^*(\cdot)$ è dato da 
\begin{equation}
\label{stimatoremmse}
g^*(Y)=E[X|Y]
\end{equation}
e l’associato MMSE è dato da

\begin{equation}
\begin{split}
&E[(X-g^*(Y))^T(X-g^*(Y))|Y]\\
&=tr(var(X|Y)) \\
&=E[X^TX|Y]-E^T[X|Y]E[X|Y]
\end{split}
\end{equation}
\textit{Dimostrazione} - Lo stimatore MMSE deve minimizzare, rispetto a tutti gli stimatori $g^*(\cdot)$, il funzionale di costo\\
\begin{equation}
\begin{split}
V(g)&=E[(X-g(Y))^T(X-g(Y))|Y]= \\
&=E[X^TX+g^T(Y)g(Y)-X^Tg(Y)-g^T(Y)X|Y)]= \\
&=E[X^TX|Y]+g^T(Y)g(Y)-E^T[X|Y]g(Y)-g^T(Y)E[X|Y]=\\
&=\underbrace{(g(Y)-E[X|Y])^T(g(Y)-E[X|Y])}_{V_1(g)}+\underbrace{E[X^TX|Y]-E^T[X|Y]E[X|Y]}_{V_2}\\
\end{split}
\end{equation}
dove il primo termine $V_1(g)$, dipendente da $g^*(\cdot)$, può essere reso nullo (quindi minimo, essendo non-negativo) scegliendo $g(Y)=g^*(Y)$ uguale alla media condizionata come in \eqref{stimatoremmse}, mentre il secondo termine $V_2$, indipendente da $g^*(\cdot)$, coincide con la traccia della varianza condizionata $tr(var(X|Y))=tr(E[XX^T|Y]-E[X|Y]E^T[X|Y])=E[X^TX|Y]-E^T[X|Y]E[X|Y]\ge 0 $ e rappresenta il costo minimo (MMSE) $Vg^*(\cdot)$.\\

\subsubsection{Stima MMSE nel caso Gaussiano} 
Se $X$ e $Y$ sono congiuntamente Gaussiane (vedi \eqref{congiuntgauss}) allora lo stimatore MMSE è dato da
\begin{equation}
\begin{split}
\widehat{X}&=g^*(Y)=E[X|Y]=\\
&=\Sigma_{XY}\Sigma_Y^{-1}Y + \mu_{X}-\Sigma_{XY}\Sigma_Y^{-1}\mu_{Y}
\end{split}
\end{equation}
con relativo MMSE dato da:
\begin{equation}
tr(\widehat{\Sigma}_X)\triangleq tr(E[(X-g^*(Y))(X-g^*(Y))^T])=tr(\Sigma_X-\Sigma_{XY}\Sigma_Y^{-1}\Sigma_{XY}^T)
\end{equation}
$Dimostrazione $ - Nelle ipotesi fatte, sfruttando le formule della media e covarianza condizionata di variabili aleatorie Gaussiane, si ha
\begin{equation}
\begin{split}
\widehat{X}&=g^*(Y)=E[X|Y]=\\
&=\mu_{X}+ \Sigma_{XY}\Sigma_Y^{-1}(Y-y^{-1})=\\
&=\Sigma_{XY}\Sigma_Y^{-1}Y+[\mu_{X}-\Sigma_{XY}\Sigma_Y^{-1}\mu_{Y}]\\
\end{split}
\end{equation}
Inoltre,
\begin{equation}
\widehat{\Sigma}_X=var(X|Y)=\Sigma_X- \Sigma_{XY}\Sigma_Y^{-1}\Sigma_{XY}^T 
\end{equation}
come volevasi dimostrare.

Si può osservare come nel caso gaussiano lo stimatore MMSE sia a tutti gli effetti uno stimatore affine, ovvero della forma $g(Y)=AY+b$, dove:
\begin{align}
A&=\Sigma_{XY}\Sigma_Y^{-1}\\
b&=\mu_{X}-A\mu_{Y}=\mu_{X}-\Sigma_{XY}\Sigma_Y^{-1}\mu_{Y}
\end{align}

In generale lo stimatore MMSE non è affine, infatti esiste il problema della ricerca del miglior stimatore affine detto stimatore BLUE (\emph{Best Linear Unbiased Estimator} con una imprecisione terminologica essendo lo stimatore cercato affine e non lineare) che è meno preciso dello stimatore MMSE.\\
Nel caso gaussiano tuttavia essi coincidono, ovvero lo stimatore MMSE appena determinato è esso stesso lo stimatore BLUE per $X$ sulla base delle osservazioni di $Y$.
\newpage