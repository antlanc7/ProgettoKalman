\section{Cenni di teoria della probabilità}

Il filtro di Kalman è un algoritmo che mira alla ricostruzione dello stato interno di un sistema basandosi unicamente su una serie di misurazioni che, a causa di limiti costruttivi, sono soggette a rumore.

A causa della natura del problema, risulta necessario affrontare alcuni aspetti della teoria della probabilità, in particolare ci soffermeremo sul concetto di variabile aleatoria normale, o Gaussiana, con l'intento di fornire un modello matematico per gli errori di misura che siamo costretti ad affrontare.

\subsection{Variabili aleatorie o casuali.}

Una variabile casuale/aleatoria è una variabile che può assumere valori diversi in dipendenza da qualche fenomeno aleatorio.
In particolare diremo che una variabile casuale $X$ si dice continua se esiste una funzione $f(x)$ definita su tutto $\mathbb{R}$ : $P(X \in B) = \int_B f(x) dx$ dove la funzione $f$ si dice \textit{densità di probabilità} della variabile casuale $X$.

L'integrale di tale funzione nel dominio di integrazione $B$ rappresenta pertanto la probabilità che la variabile aleatoria assuma valori appartenenti a $B$.

Le variabili casuali risultano essere un valido strumento matematico per la modellazione dei rumori.

Alle variabili casuali sono associati i concetti di media/valore atteso, di varianza e di covarianza.

\subsubsection{Valore atteso}

Nella teoria della probabilità il valore atteso di una variabile casuale $X$, è un numero indicato con $E[X]$ che formalizza l'idea di valore medio di un fenomeno aleatorio.
\begin{equation}
\ {\mathbb  {E}}[X]=\int _{{-\infty }}^{{\infty }}xf(x)dx
\end{equation}

\noindent  Si noti che l'operatore valore atteso è lineare:
\begin{equation}
E[aX + bY] = aE[X] + bE[Y]
\end{equation}


\subsubsection{Varianza}

La varianza di una variabile aleatoria è una funzione che fornisce una misura della variabilità dei valori assunti dalla variabile stessa; nello specifico, la misura di quanto essi si discostino dal valore atteso.

La varianza della variabile aleatoria $X$ è definita come il valore atteso del quadrato della variabile aleatoria centrata sulla propria media: $X - E[X]$ :
\begin{equation}
Var(X) = E[(X - E[X])^2]
\end{equation}

\subsubsection{Covarianza}

La covarianza di due variabili aleatorie è un numero che fornisce una misura di quanto le due varino dipendentemente l'una dall'altra.

La covarianza di due variabili aleatorie $X$ e $Y$ è il valore atteso dei prodotti delle loro distanze dalla media: \begin{equation}
Cov(X,Y)= E [(X-E[X])(Y-E[Y])]
\end{equation}
Due variabili casuali si dicono \textit{incorrelate} se la loro covarianza è nulla.

La covarianza può essere considerata una generalizzazione della varianza:
\begin{equation}
Var(X) = Cov(X,X)
\end{equation}

\subsection{Vettori casuali}

Un vettore casuale è un vettore i cui elementi sono variabili casuali.\\
Risulta necessario estendere le definizioni date in precedenza per caratterizzare rumori che agiscono su sistemi non scalari.

\subsubsection{Valore atteso}

Si dice valore atteso del vettore casuale $X \in \mathbb{R}^n$ il vettore dei valori attesi delle variabili casuali che lo compongono: 
\begin{equation}
E[X] = \begin{pmatrix}
E[X_1] & E[X_2] & \dots & E[X_n]
\end{pmatrix}^T
\end{equation}
Si definisce \textit{valore quadratico medio} di $X$ come $E[X^T X]$.

\subsubsection{Matrice di covarianza}

Si definisce \textit{matrice di covarianza} del vettore casuale $X \in \mathbb{R}^n$ la matrice $n \times n$: \begin{equation}
 \Sigma_X = Cov(X, X) = E[(X-E[X])(X-E[X])^T]
\end{equation}
Per come è definita, la matrice di covarianza è una matrice simmetrica semidefinita positiva i cui elementi $\Sigma_{ij}$ sono le covarianze tra gli elementi $X_i$ e $X_j$ del vettore $X$.

A sua volta si definisce la matrice di \textit{cross-covarianza} tra due vettori casuali $X$ e $Y$, la matrice
\begin{equation}
 \Sigma_{XY} = Cov(X, Y) = E[(X-E[X])(Y-E[Y])^T]
\end{equation}
Due vettori casuali $X$ e $Y$ si dicono \textit{incorrelati} se $\Sigma_{XY} = 0$.\\
Dati due vettori casuali $X$ e $Y$ l'operazione di \textit{congiunzione} definisce un nuovo vettore casuale $v=\begin{pmatrix}X \\ Y\end{pmatrix}$ tale che: 
\begin{equation}
E[v]=\begin{pmatrix}\mu_X \\ \mu_Y\end{pmatrix} \qquad \Sigma_v=\begin{pmatrix}\Sigma_X && \Sigma_{XY} \\ \Sigma_{YX} && \Sigma_Y\end{pmatrix}
\end{equation}
Si definisce inoltre l'operazione di condizionamento tra i vettori $X$ e $Y$: essa produce una variabile aleatoria $X|Y$ delle stesse dimensioni di $X$ che modellizza il comportamento della variabile casuale $X$ in seguito all'osservazione del valore assunto dalla variabile $Y$.

\subsection{Variabili gaussiane}

Le variabili gaussiane sono particolari variabili aleatorie caratterizzate da due parametri, $\mu$ e $\sigma^2$, e sono indicate tradizionalmente con: 
\begin{equation}
X \sim N(\mu ,\sigma^2)
\end{equation}
Sono caratterizzate dalla funzione densità di probabilità: 
\begin{equation}
f(x) = \frac{1}{\sqrt {2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}
Si può dimostrare che per le variabili gaussiane vale che: 
\begin{equation}
E[X]= \mu \qquad Var[X]= \sigma^2
\end{equation}
Possiamo definire \textit{vettore casuale gaussiano} un vettore di variabili gaussiane.\\
Allo stesso modo delle variabili gaussiane scalari, un vettore gaussiano $X$ è descritto completamente dalla media $\mu_X \in \mathbb{R}^n$ e dalla matrice di covarianza $\Sigma_X \in \mathbb{R}^{n \times n}$.\\

\subsubsection{Condizionamento di variabili gaussiane}
Siano $X$ e $Y$ due variabili aleatorie congiuntamente gaussiane, ovvero tali che:
\begin{equation}
\label{congiuntgauss}
\begin{pmatrix}X \\ Y\end{pmatrix} \sim \mathcal{N} \Biggl ( \begin{pmatrix}\mu_X \\ \mu_Y\end{pmatrix}, \begin{pmatrix}\Sigma_X && \Sigma_{XY} \\ \Sigma_{YX} && \Sigma_Y\end{pmatrix} \Biggl )
\end{equation}
Allora il condizionamento tra le due variabili $X|Y$ è a sua volta una variabile gaussiana.
Si dimostra\cite{stimablue} che media e covarianza di tale variabile valgono:
\begin{align}
\mu_{X|Y}&=\mu_X + \Sigma_{XY}\Sigma_{Y}^{-1}(y-\mu_Y)\\
\Sigma_{X|Y}&=\Sigma_X-\Sigma_{XY}\Sigma_{Y}^{-1}\Sigma_{YX}
\end{align}
dove $y$ rappresenta il valore dell'osservazione della variabile $Y$.

\newpage